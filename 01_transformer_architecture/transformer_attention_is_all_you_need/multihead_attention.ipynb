{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f495f0f6",
   "metadata": {},
   "source": [
    "# Implementing Multi-head Attention\n",
    "\n",
    "### Ref: [The AiEdge Newsletter](https://drive.google.com/file/d/1Je2SAFBlsWcgwzK_gl1_f-LtPK3SOzg3/view)\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"../../assets/multihead_attention.png\" width=\"500\" height=\"250\">\n",
    "  <img src=\"../../assets/attention_head.png\" width=\"500\" height=\"250\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f07243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14884f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of multi-head attention that is used in transformer architecture.\n",
    "    It splits the input (embeddings) into multiple heads, computes attention for each \n",
    "    for each head, and then concatenate the results.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input (embedding) dimension (also known as hidden_size)\n",
    "        n_head (int): number of attention heads\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if d_model is not divisible by n_head\n",
    "    \"\"\"    \n",
    "    def __init__(self, d_model: int, n_head: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if d_model % n_head != 0: \n",
    "            raise ValueError('d_model should be divisible by n_head ...')\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // n_head     # head dimension\n",
    "\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.O = nn.Linear(d_model, d_model) # allows the model to remix the heads that optimizes task performance.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        compute multi-head self-attention\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor of size [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor of shape [batch_size, seq_len, d_model]\n",
    "        \"\"\"        \n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # linear projection of input\n",
    "        queries = self.Q(x) # [batch_size, seq_len, d_model]\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "\n",
    "        # split into multiple heads (by reshaping tensors).\n",
    "        # each of the following tensors has [batch_size, n_head, seq_len, d_head] shape.\n",
    "        keys = keys.reshape((batch_size, seq_len, self.n_head, self.d_head)).transpose(1, 2)\n",
    "        queries = queries.reshape((batch_size, seq_len, self.n_head, self.d_head)).transpose(1, 2)\n",
    "        values = values.reshape((batch_size, seq_len, self.n_head, self.d_head)).transpose(1, 2)\n",
    "\n",
    "        # compute self-attention\n",
    "        scores = queries @ keys.transpose(-1, -2) / self.d_head ** 0.5  \n",
    "        self_attn = F.softmax(scores, dim=-1)   # [batch_size, n_head, seq_len, seq_len]\n",
    "        out = self_attn @ values    # [batch_size, n_head, seq_len, d_head]\n",
    "\n",
    "        # recombine heads\n",
    "        out = out.transpose(1, 2)   # [batch_size, seq_len, n_head, d_head]\n",
    "        out = out.reshape((batch_size, seq_len, d_model))\n",
    "\n",
    "        # final linear projection\n",
    "        out = self.O(out)   # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0427f",
   "metadata": {},
   "source": [
    "#### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d42bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x: torch.Size([2, 5]), and embedded: torch.Size([2, 5, 12]), and hidden state: torch.Size([2, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "vocab_size = 15\n",
    "d_model = 12    # or model dim or hidden size\n",
    "seq_len = 5\n",
    "n_head = 3\n",
    "\n",
    "# generate random Token Ids (note that Embedding layer only receives integer values)\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "embedded = embedding(x)     # [batch_size, seq_len, d_model]\n",
    "\n",
    "attn = MultiHeadAttention(d_model, n_head)\n",
    "hidden_state = attn(embedded)   # [batch_size, seq_len, d_model]\n",
    "\n",
    "print(f'input x: {x.size()}, and embedded: {embedded.size()}, and hidden state: {hidden_state.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a7e7f",
   "metadata": {},
   "source": [
    "## More Efficient implementation of MultiHeadAttention\n",
    "* It is faster and more memory-efficient on GPU (by using one big linear projection for Q, K, and V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input (embedding) dimension (also known as hidden_size)\n",
    "        n_head (int): number of attention heads\n",
    "    Raises:\n",
    "        ValueError: if d_model is not divisible by n_hea\n",
    "    \"\"\"    \n",
    "    def __init__(self, d_model: int, n_head: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if d_model % n_head != 0: \n",
    "            raise ValueError('d_model should be divisible by n_head ...')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head   # head dimension\n",
    "\n",
    "        # combined linear projection for Q, K, and V\n",
    "        self.qkv_linear = nn.Linear(d_model, d_model * 3)\n",
    "        # Final output linear projection that allows the model to remix the heads that optimizes task performance.\n",
    "        self.O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to compute multi-head self attention\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor of shape [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: hedden states tensor with shape of [batch_size, seq_len, d_model]\n",
    "        \"\"\"        \n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv_linear(x)    # [batch_size, seq_len, d_model * 3]\n",
    "        qkv = qkv.reshape((batch_size, seq_len, self.n_head, self.d_head * 3))\n",
    "        qkv = qkv.transpose(1, 2)   # [batch_size, n_head, seq_len, d_head * 3]\n",
    "\n",
    "        queries, keys, values = qkv.chunk(3, dim=-1)    # each has dimension of [batch_size, n_head, seq_len, d_head]\n",
    "        scores = queries @ keys.transpose(-1, -2) / self.d_head ** 0.5\n",
    "        self_attn = F.softmax(scores, dim=-1)  # [batch_size, n_head, seq_len, seq_len]\n",
    "\n",
    "        out = self_attn @ values    # [batch_size, n_head, seq_len, d_head]\n",
    "        out = out.transpose(1, 2)   # [batch_size, seq_len, n_head, d_head]\n",
    "        out = out.reshape((batch_size, seq_len, d_model))\n",
    "        out = self.O(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17044942",
   "metadata": {},
   "source": [
    "#### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be833816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x: torch.Size([2, 5]), and embedded: torch.Size([2, 5, 12]), and hidden state: torch.Size([2, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "vocab_size = 15\n",
    "d_model = 12    # or model dim or hidden size\n",
    "seq_len = 5\n",
    "n_head = 3\n",
    "\n",
    "# generate random Token Ids (note that Embedding layer only receives integer values)\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "embedded = embedding(x)     # [batch_size, seq_len, d_model]\n",
    "\n",
    "attn = MultiHeadAttentionV2(d_model, n_head)\n",
    "hidden_state = attn(embedded)   # [batch_size, seq_len, d_model]\n",
    "\n",
    "print(f'input x: {x.size()}, and embedded: {embedded.size()}, and hidden state: {hidden_state.size()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
