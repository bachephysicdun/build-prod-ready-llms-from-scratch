{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cd1747",
   "metadata": {},
   "source": [
    "# Positional Encoding in Attention is All You Need (AAYN)\n",
    "\n",
    "#### $PE_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$ and $PE_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n",
    "\n",
    "### Ref: [The AiEdge Newsletter](https://drive.google.com/file/d/1Je2SAFBlsWcgwzK_gl1_f-LtPK3SOzg3/view)\n",
    "\n",
    "<img src=\"../../assets/pos_enc.png\" width=\"700\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a1cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    simple positional encoding with transformer model (in attention is all you need)\n",
    "\n",
    "    Args:\n",
    "        context_size (int): maximum lenght of the input sequence (also known as max_length)\n",
    "        d_model (int): internal dimension of the model or dimension of embeddings.\n",
    "        (also known as 'hidden_size')\n",
    "    \"\"\"    \n",
    "    def __init__(self, context_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.arange(0, context_size).unsqueeze(dim=1) # [context_size, 1]\n",
    "        # dimension indices\n",
    "        # for d_model=5 -> ii = (0, 2, 4) and ii[:d_model//2] = (0, 2) (see the figure above)\n",
    "        # this way of implementation, will cover both even and odd values for d_model\n",
    "        ii = torch.arange(0, (d_model+1)//2)\n",
    "        div = 10000 ** (2*ii/d_model)\n",
    "        \n",
    "        # initialize positional encoding [context_size, d_model]\n",
    "        self.encoding = torch.zeros(context_size, d_model)\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / div) # even positions\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / div[:d_model//2]) # odd  positions\n",
    "\n",
    "        # Registers positional encoding tensor as part of the module state, but not as a \n",
    "        # learnable parameter (i.e., not updated by gradient descent). Positional encodings \n",
    "        # in the vanilla “Attention Is All You Need” is not trained.\n",
    "        # Moreover, when register as buffer, it moves with model and is saved in state_dict\n",
    "        self.register_buffer('pos_encoding', self.encoding)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        returns positional encoding for a given input tensor x.\n",
    "        (input tensor x is comming from token embedding layer in the transformer architecture)\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: positional encoding slice of shape [seq_len, d_model], \n",
    "            ready to be added to token embeddings.\n",
    "        \"\"\"        \n",
    "        seq_len = x.size(1) # number of tokens in the input sequence\n",
    "        \n",
    "        # make sure to use 'pos_encoding' from self.register_buffer \n",
    "        # (otherwise we can't use the benefits of self.register_buffer)\n",
    "        return self.pos_encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17860d",
   "metadata": {},
   "source": [
    "#### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b53fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15\n",
    "d_model = 11\n",
    "context_size = 20   # maximum sequence length (that is suppored)\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model) # token embedding layer\n",
    "pos_encoder = PositionalEncoding(context_size=context_size, d_model=d_model) # positional encoding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec1ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  1.8629e-01,  9.8250e-01,  3.5105e-02,\n",
       "          9.9938e-01,  6.5793e-03,  9.9998e-01,  1.2328e-03,  1.0000e+00,\n",
       "          2.3101e-04],\n",
       "        [ 9.0930e-01, -4.1615e-01,  3.6605e-01,  9.3059e-01,  7.0166e-02,\n",
       "          9.9754e-01,  1.3158e-02,  9.9991e-01,  2.4657e-03,  1.0000e+00,\n",
       "          4.6203e-04],\n",
       "        [ 1.4112e-01, -9.8999e-01,  5.3300e-01,  8.4611e-01,  1.0514e-01,\n",
       "          9.9446e-01,  1.9737e-02,  9.9981e-01,  3.6985e-03,  9.9999e-01,\n",
       "          6.9304e-04],\n",
       "        [-7.5680e-01, -6.5364e-01,  6.8129e-01,  7.3201e-01,  1.3999e-01,\n",
       "          9.9015e-01,  2.6314e-02,  9.9965e-01,  4.9314e-03,  9.9999e-01,\n",
       "          9.2405e-04],\n",
       "        [-9.5892e-01,  2.8366e-01,  8.0573e-01,  5.9228e-01,  1.7466e-01,\n",
       "          9.8463e-01,  3.2891e-02,  9.9946e-01,  6.1642e-03,  9.9998e-01,\n",
       "          1.1551e-03],\n",
       "        [-2.7942e-01,  9.6017e-01,  9.0196e-01,  4.3182e-01,  2.0912e-01,\n",
       "          9.7789e-01,  3.9466e-02,  9.9922e-01,  7.3970e-03,  9.9997e-01,\n",
       "          1.3861e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: batch of 2 sequences with 7 elements\n",
    "x = torch.tensor([\n",
    "    [6, 7, 8, 9, 0, 1, 2],\n",
    "    [0, 0, 1, 4, 5, 9, 5]\n",
    "]) # shape [batch_size, seq_len]\n",
    "\n",
    "x_emb = embedding(x) #[batch_size, seq_len, d_model]\n",
    "x_pos_embed = pos_encoder(x_emb) #[seq_len, d_model]\n",
    "x_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0dd939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 7, 11]), torch.Size([7, 11]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_emb.size(), x_pos_embed.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
