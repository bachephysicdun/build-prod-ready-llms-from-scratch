{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cd1747",
   "metadata": {},
   "source": [
    "# Positional Encoding in Attention is All You Need (AAYN)\n",
    "\n",
    "* $PE_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$ and $PE_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n",
    "\n",
    "**Ref: [The AiEdge Newsletter](https://drive.google.com/file/d/1Je2SAFBlsWcgwzK_gl1_f-LtPK3SOzg3/view)**\n",
    "\n",
    "<img src=\"../../assets/pos_enc.png\" width=\"700\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a1cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521a4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    simple positional encoding with transformer model (in attention is all you need)\n",
    "\n",
    "    Args:\n",
    "        context_size (int): maximum lenght of the input sequence (also known as max_length)\n",
    "        d_model (int): internal dimension of the model or dimension of embeddings.\n",
    "        (also known as 'hidden_size')\n",
    "    \"\"\"    \n",
    "    def __init__(self, context_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.arange(0, context_size).unsqueeze(dim=1) # [context_size, 1]\n",
    "        # even dimension indices\n",
    "        ii = torch.arange(0, d_model, 2)\n",
    "        \n",
    "        # initialize positional encoding [context_size, d_model]\n",
    "        self.encoding = torch.zeros(context_size, d_model)\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / 10000 ** (ii/d_model)) # even positions\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / 10000 ** (ii/d_model)) # odd  positions\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        returns positional encoding for a given input tensor x.\n",
    "        (input tensor x is comming from token embedding layer in the transformer architecture)\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: positional encoded tensor of shape [seq_len, d_model]\n",
    "        \"\"\"        \n",
    "        seq_len = x.size(1) # number of tokens in the input sequence\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b53fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15\n",
    "d_model = 10\n",
    "context_size = 20   # maximum sequence length (that is suppored)\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model) # token embedding layer\n",
    "pos_encoder = PositionalEncoding(context_size=context_size, d_model=d_model) # positional encoding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec1ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
       "          9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
       "          9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
       "          9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
       "          9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00],\n",
       "        [-9.5892e-01,  2.8366e-01,  7.1207e-01,  7.0211e-01,  1.2526e-01,\n",
       "          9.9212e-01,  1.9904e-02,  9.9980e-01,  3.1548e-03,  1.0000e+00],\n",
       "        [-2.7942e-01,  9.6017e-01,  8.1396e-01,  5.8092e-01,  1.5014e-01,\n",
       "          9.8866e-01,  2.3884e-02,  9.9971e-01,  3.7857e-03,  9.9999e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: batch of 2 sequences with 7 elements\n",
    "x = torch.tensor([\n",
    "    [6, 7, 8, 9, 0, 1, 2],\n",
    "    [0, 0, 1, 4, 5, 9, 5]\n",
    "]) # shape [batch_size, seq_len]\n",
    "\n",
    "x_emb = embedding(x) #[batch_size, seq_len, d_model]\n",
    "x_pos_embed = pos_encoder(x_emb) #[seq_len, d_model]\n",
    "x_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0dd939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 7, 10]), torch.Size([7, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_emb.size(), x_pos_embed.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
